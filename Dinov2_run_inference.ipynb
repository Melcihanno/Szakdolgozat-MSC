{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47144c3a-7df8-4e97-b94a-dc4ebff2d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import SemanticSegmenterOutput  \n",
    "from transformers import Dinov2Model, Dinov2PreTrainedModel  \n",
    "from retouch_dataloader_utils import load_train_and_val  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from aroi_dataloder_utils import load_val\n",
    "from torch.utils.data import DataLoader  \n",
    "from torch.utils.data import Dataset  \n",
    "import torch.nn.functional as F  \n",
    "from torch.optim import AdamW  \n",
    "from tqdm.auto import tqdm  \n",
    "import albumentations as A \n",
    "from PIL import Image  \n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import torch  \n",
    "import cv2 \n",
    "import csv \n",
    "import os \n",
    "\n",
    "\n",
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9062168-888d-4bc4-a865-f039a8597cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset): \n",
    "  def __init__(self, dataset, transform): \n",
    "    self.dataset = dataset \n",
    "    self.transform = transform \n",
    " \n",
    "  def __len__(self): \n",
    "    return len(self.dataset) \n",
    " \n",
    "  def __getitem__(self, idx): \n",
    "    item = self.dataset[idx] \n",
    "\n",
    "    original_image= np.load(item[\"image_path\"])\n",
    "    original_image = np.stack([original_image] * 3, axis=-1)\n",
    "\n",
    "    original_segmentation_map = np.load(item[\"label_path\"])\n",
    "    transformed = self.transform(image=original_image, mask=original_segmentation_map)\n",
    "    image, target = torch.tensor(transformed['image']), torch.LongTensor(transformed['mask']) \n",
    " \n",
    "    image = image.permute(2, 0, 1)\n",
    "    image_path=item[\"image_path\"] \n",
    "      \n",
    "    return image, target, original_image, original_segmentation_map, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274d2f6-3da0-4233-b944-ef49ed58d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(inputs): \n",
    "    batch = dict() \n",
    "    batch[\"pixel_values\"] = torch.stack([i[0] for i in inputs], dim=0) \n",
    "    batch[\"labels\"] = torch.stack([i[1] for i in inputs], dim=0) \n",
    "    batch[\"original_images\"] = [i[2] for i in inputs] \n",
    "    batch[\"original_segmentation_maps\"] = [i[3] for i in inputs]\n",
    "    batch[\"image_path\"] = [i[4] for i in inputs]\n",
    " \n",
    "    return batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941db09e-6eb5-4921-8328-c3518f55b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module): \n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1): \n",
    "        super(LinearClassifier, self).__init__() \n",
    " \n",
    "        self.in_channels = in_channels \n",
    "        self.width = tokenW \n",
    "        self.height = tokenH \n",
    "\n",
    "        #Két-réteg:\n",
    "        \n",
    "        #self.conv1 = torch.nn.Conv2d(in_channels, 64, (6,6), padding=1)\n",
    "        #self.conv2 = torch.nn.Conv2d(64, 128, (6,6), padding=1)\n",
    "        #self.classifier = torch.nn.Conv2d(128, num_labels, (1,1))\n",
    "\n",
    "        #Egy réteg:\n",
    "        self.conv = torch.nn.Conv2d(in_channels, 128, (6,6), padding=1)\n",
    "        self.classifier = torch.nn.Conv2d(128, num_labels, (1,1))\n",
    "\n",
    "    def forward(self, embeddings): \n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels) \n",
    "        embeddings = embeddings.permute(0,3,1,2) \n",
    "\n",
    "        #Két-réteg:\n",
    "        #x = torch.relu(self.conv1(embeddings))\n",
    "        #x = torch.relu(self.conv2(x))\n",
    "        #return self.classifier(x)\n",
    "        \n",
    "        #Egy-réteg\n",
    "        x = torch.relu(self.conv(embeddings))\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee692f37-681d-4050-9a48-77726ea78f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel): \n",
    "  def __init__(self, config): \n",
    "    super().__init__(config) \n",
    " \n",
    "    self.dinov2 = Dinov2Model(config) \n",
    "    self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels) \n",
    " \n",
    "  def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None): \n",
    "    outputs = self.dinov2(pixel_values, \n",
    "                            output_hidden_states=output_hidden_states, \n",
    "                            output_attentions=output_attentions) \n",
    "    patch_embeddings = outputs.last_hidden_state[:,1:,:] \n",
    " \n",
    "    logits = self.classifier(patch_embeddings) \n",
    "    logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False) \n",
    " \n",
    "    loss = None \n",
    "    if labels is not None: \n",
    "      loss_fct = torch.nn.CrossEntropyLoss(ignore_index=0) \n",
    "      loss = loss_fct(logits.squeeze(), labels.squeeze()) \n",
    " \n",
    "    return SemanticSegmenterOutput( \n",
    "        loss=loss, \n",
    "        logits=logits, \n",
    "        hidden_states=outputs.hidden_states, \n",
    "        attentions=outputs.attentions, \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97b6ee-1ea3-41e2-ba87-377ef588b12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbdbe7-5fab-4eb9-a989-633495ab73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_number(image_dir,path):\n",
    "    new_elem=path.replace(image_dir,'')\n",
    "    elem_list=new_elem.split('/')\n",
    "    return int(elem_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e727ab6-8cfb-4141-b715-e976013d3c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_index(image_dir,path):\n",
    "    new_elem=path.replace(image_dir,'')\n",
    "    elem_list=new_elem.split('/')\n",
    "    elem_list=elem_list[2].replace('.npy','')\n",
    "    elem_list=elem_list.split('_')\n",
    "    elem_list=elem_list[1]\n",
    "    return int(elem_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d22cf-e1f1-48a4-b2c2-efaa292965a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice_for_inference_3d(preds_3d, targets_3d, num_classes):\n",
    "    preds_3d = preds_3d.unsqueeze(0).long()\n",
    "    targets_3d = targets_3d.unsqueeze(0).long()\n",
    "\n",
    "    preds_one_hot = torch.nn.functional.one_hot(preds_3d, num_classes=num_classes)\n",
    "    targets_one_hot = torch.nn.functional.one_hot(targets_3d, num_classes=num_classes)\n",
    "\n",
    "    preds_one_hot = preds_one_hot.permute(0, 4, 1, 2, 3).float()\n",
    "    targets_one_hot = targets_one_hot.permute(0, 4, 1, 2, 3).float()\n",
    "\n",
    "    intersection = (preds_one_hot * targets_one_hot).sum(dim=(2, 3, 4))\n",
    "    union = preds_one_hot.sum(dim=(2, 3, 4)) + targets_one_hot.sum(dim=(2, 3, 4))\n",
    "\n",
    "    dice = (2.0 * intersection) / (union + 1e-6)\n",
    "\n",
    "    return dice.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf397b-8146-45d1-8998-2c6e05aa7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model and run inference on it\n",
    "def run_3d_inference_on_models(epoch,learning_rate,cross_validation,path,image_dir,label_dir):\n",
    "    path=path+str(epoch)+'_'+str(learning_rate)+'_'+str(cross_validation)+'.pth'\n",
    "    \n",
    "    model = Dinov2ForSemanticSegmentation.from_pretrained(\"facebook/dinov2-base\", id2label=id2label, num_labels=len(id2label))\n",
    "    model.load_state_dict(torch.load(path, weights_only=True))\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    dataset = load_train_and_val(image_dir,label_dir,5,cross_validation)\n",
    "    \n",
    "    for name, param in model.named_parameters(): \n",
    "      if name.startswith(\"dinov2\"): \n",
    "        param.requires_grad = False \n",
    "    \n",
    "    val_dataset = SegmentationDataset(dataset[\"validation\"], transform=val_transform)\n",
    "    \n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1, pin_memory=True,collate_fn=collate_fn)\n",
    "    \n",
    "    image_dict={}\n",
    "    label_dict={}\n",
    "    maxdict={}\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        scan_number = get_scan_number(image_dir,batch[\"image_path\"][0])\n",
    "        scan_index = get_scan_index(image_dir,batch[\"image_path\"][0])\n",
    "        key=str(scan_number)+'_'+str(scan_index)\n",
    "        image_dict[key]=batch[\"pixel_values\"]\n",
    "        label_dict[key]=batch[\"labels\"]\n",
    "    \n",
    "        if scan_number not in maxdict:\n",
    "            maxdict[scan_number] = scan_index\n",
    "        else:\n",
    "            maxdict[scan_number] = max(maxdict[scan_number], scan_index)\n",
    "            \n",
    "    model = model.to(device)\n",
    "    \n",
    "    columns = [\"scan_number\",\"Background\", \"IRF\", \"SRF\", \"PED\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for idx,key in enumerate(maxdict.keys()):\n",
    "        value=maxdict[key]\n",
    "        preds_3d = []\n",
    "        labels_3d = []\n",
    "        for val in range(value+1):\n",
    "            keyval=str(key)+'_'+str(val)\n",
    "            test_image = image_dict[keyval]\n",
    "            labels = label_dict[keyval]\n",
    "            with torch.no_grad():\n",
    "                outputs = model(test_image.to(device))\n",
    "                size=test_image.shape[:2]\n",
    "                logits = outputs.logits\n",
    "                preds = logits.argmax(dim=1)\n",
    "                preds_3d.append(preds.squeeze(0).cpu())\n",
    "                labels_3d.append(labels.squeeze(0).cpu())\n",
    "    \n",
    "        preds_3d = torch.stack(preds_3d, dim=0)\n",
    "        labels_3d = torch.stack(labels_3d, dim=0)\n",
    "                \n",
    "        dice_score = compute_dice_for_inference_3d(preds_3d.detach().cpu(), labels_3d.detach().cpu(), num_classes=len(id2label))\n",
    "        dice_list = [round(val, 4) for val in dice_score.squeeze().tolist()]\n",
    "        df.loc[idx] = [key,dice_list[0],dice_list[1],dice_list[2],dice_list[3]]\n",
    "        \n",
    "    csv_filename=str(epoch)+'_'+str(learning_rate)+'_'+str(cross_validation)+'_3d_inference.csv'\n",
    "    print(csv_filename,' Is ready!')\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7e040-26b2-45ea-8fb0-4f13eb728a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0:\"Background\",\n",
    "    1:\"IRF\",\n",
    "    2:\"SRF\",\n",
    "    3:\"PED\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22448d42-a38e-4b47-96e4-45aacac4d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = (np.array([123.675, 116.280, 103.530])).tolist()\n",
    "ADE_STD = (np.array([58.395, 57.120, 57.375])).tolist()\n",
    "\n",
    "val_transform = A.Compose([ \n",
    "    A.Resize(width=448, height=448), \n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD), \n",
    "], is_check_shapes=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0bbb8-2c34-4857-aba3-b716d00f352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='.../modellek/ \n",
    "image_dir = '.../RETOUCH_TRAINING/imagesTr'\n",
    "label_dir = '.../RETOUCH_TRAINING/labelsTr'\n",
    "\n",
    "\n",
    "epochs=[5,10,100]\n",
    "learning_rates=['1e-2','1e-3','1e-4','1e-5']\n",
    "cross_validation=5\n",
    "\n",
    "for epoch in epochs:\n",
    "    for lr in learning_rates:\n",
    "        for cross_val in range(cross_validation):\n",
    "            print(cross_val)\n",
    "            run_3d_inference_on_models(epoch,lr,cross_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
